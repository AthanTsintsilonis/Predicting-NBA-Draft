{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e732b4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests import get\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import os.path\n",
    "from os import path\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14d66765",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling URLs to scrape\n",
    "years = list(range(2013, 2022))\n",
    "years_custom = list(range(2013, 2020))\n",
    "url_big12 = 'https://www.sports-reference.com/cbb/conferences/big-12/{}-stats.html'\n",
    "url_sec = 'https://www.sports-reference.com/cbb/conferences/sec/{}-stats.html'\n",
    "url_big10 = 'https://www.sports-reference.com/cbb/conferences/big-ten/{}-stats.html'\n",
    "url_bigeast = 'https://www.sports-reference.com/cbb/conferences/big-east/{}-stats.html'\n",
    "url_pac12 = 'https://www.sports-reference.com/cbb/conferences/pac-12/{}-stats.html'\n",
    "url_acc = 'https://www.sports-reference.com/cbb/conferences/acc/{}-stats.html'\n",
    "url_aac = 'https://www.sports-reference.com/cbb/conferences/aac/{}-stats.html'\n",
    "url_mwc = 'https://www.sports-reference.com/cbb/conferences/mwc/{}-stats.html'\n",
    "url_wcc = 'https://www.sports-reference.com/cbb/conferences/wcc/{}-stats.html'\n",
    "url_a10 = 'https://www.sports-reference.com/cbb/conferences/atlantic-10/{}-stats.html'\n",
    "url_mvc = 'https://www.sports-reference.com/cbb/conferences/mvc/{}-stats.html'\n",
    "url_cusa = 'https://www.sports-reference.com/cbb/conferences/cusa/{}-stats.html'\n",
    "url_col = 'https://www.sports-reference.com/cbb/conferences/colonial/{}-stats.html'\n",
    "url_south = 'https://www.sports-reference.com/cbb/conferences/southern/{}-stats.html'\n",
    "url_wac = 'https://www.sports-reference.com/cbb/conferences/wac/{}-stats.html'\n",
    "url_maac = 'https://www.sports-reference.com/cbb/conferences/maac/{}-stats.html'\n",
    "url_bw = 'https://www.sports-reference.com/cbb/conferences/big-west/{}-stats.html'\n",
    "url_sb = 'https://www.sports-reference.com/cbb/conferences/sun-belt/{}-stats.html'\n",
    "url_ivy = 'https://www.sports-reference.com/cbb/conferences/ivy/{}-stats.html'\n",
    "url_mac = 'https://www.sports-reference.com/cbb/conferences/mac/{}-stats.html'\n",
    "url_as = 'https://www.sports-reference.com/cbb/conferences/atlantic-sun/{}-stats.html'\n",
    "url_ovc = 'https://www.sports-reference.com/cbb/conferences/ovc/{}-stats.html'\n",
    "url_summit = 'https://www.sports-reference.com/cbb/conferences/summit/{}-stats.html'\n",
    "url_big_south = 'https://www.sports-reference.com/cbb/conferences/big-south/{}-stats.html'\n",
    "url_big_sky = 'https://www.sports-reference.com/cbb/conferences/big-sky/{}-stats.html'\n",
    "url_america_east = 'https://www.sports-reference.com/cbb/conferences/america-east/{}-stats.html'\n",
    "url_horizon = 'https://www.sports-reference.com/cbb/conferences/horizon/{}-stats.html'\n",
    "url_patriot = 'https://www.sports-reference.com/cbb/conferences/patriot/{}-stats.html'\n",
    "url_southland = 'https://www.sports-reference.com/cbb/conferences/southland/{}-stats.html'\n",
    "url_northeast = 'https://www.sports-reference.com/cbb/conferences/northeast/{}-stats.html'\n",
    "url_meac = 'https://www.sports-reference.com/cbb/conferences/meac/{}-stats.html'\n",
    "url_swac = 'https://www.sports-reference.com/cbb/conferences/swac/{}-stats.html'\n",
    "url_draft =     'https://www.nbadraft.net/{}-nba-draft-combine-measurements/'\n",
    "url_draft_summary = 'https://basketball.realgm.com/nba/draft/past_drafts/{}'\n",
    "\n",
    "list_of_urls = [\n",
    "url_big12, \n",
    "url_sec, \n",
    "url_big10,\n",
    "url_bigeast,\n",
    "url_pac12,\n",
    "url_acc,\n",
    "url_aac,\n",
    "url_mwc,\n",
    "url_wcc,\n",
    "url_a10,\n",
    "url_mvc,\n",
    "url_cusa,\n",
    "url_col,\n",
    "url_south,\n",
    "url_wac,\n",
    "url_maac,\n",
    "url_bw,\n",
    "url_sb,\n",
    "url_ivy,\n",
    "url_mac,\n",
    "url_as,\n",
    "url_ovc ,\n",
    "url_summit,\n",
    "url_big_south,\n",
    "url_big_sky,\n",
    "url_america_east,\n",
    "url_horizon,\n",
    "url_patriot,\n",
    "url_southland,\n",
    "url_northeast,\n",
    "url_meac,\n",
    "url_swac\n",
    "]\n",
    "#Table ID\n",
    "NCAA_id = 'conference-stats'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19644134",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generates File name\n",
    "def generate_name(url, year):\n",
    "    end_index = url[49:].find('/')\n",
    "    name = url[49:49+end_index]+'_'+str(year)\n",
    "    return(name)\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6f36f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generates File Directory\n",
    "def generate_directory(directory, file_name):\n",
    "    generated_directory = directory + '/{}.html'.format(file_name)\n",
    "    return(generated_directory)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a168aca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloads HTML File from given URL to directory\n",
    "def get_html_request(directory, url):\n",
    "    for year in years:\n",
    "        url_temp = url.format(year)\n",
    "        data = requests.get(url_temp)\n",
    "        with open(generate_directory(directory, generate_name(url, year)), 'w+',encoding=\"utf-8\") as f:\n",
    "                f.write(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29ee924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloads HTML File from given URL to /NCAA\n",
    "def get_html_request_NCAA(url):\n",
    "    for year in years:\n",
    "        url_temp = url.format(year)\n",
    "        data = requests.get(url_temp)\n",
    "        with open('NCAA/{}.html'.format(generate_name(url, year)), 'w+',encoding=\"utf-8\") as f:\n",
    "                f.write(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "910f370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloads HTML File from given URL to /Draft\n",
    "def get_html_request_draft(url):\n",
    "    for year in years:\n",
    "        url_temp = url.format(year)\n",
    "        data = requests.get(url_temp)\n",
    "        with open('Draft/{}.html'.format(year), 'w+',encoding=\"utf-8\") as f:\n",
    "                f.write(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbe4780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloads HTML File from given URL to /NCAA, uses Selenium to execute javascript if required\n",
    "def get_html_api(url):\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    for year in years_custom:        \n",
    "        temp_url = url.format(year)\n",
    "        driver.get(temp_url)\n",
    "        driver.execute_script(\"window.scrollTo(1,10000)\")\n",
    "        time.sleep(4)\n",
    "        with open(\"NCAA/{}.html\".format(generate_name(url, year)), \"w+\",encoding=\"utf-8\") as f:\n",
    "            f.write(driver.page_source)\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5e65da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Searches webpage for all tables\n",
    "def search_url_for_tables(url):\n",
    "    r = get(url)\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "    tables = soup.find_all('table')\n",
    "    return(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8877f6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uses requests to get html and searches for all tables, returning them as a df with first row as header\n",
    "def get_df_from_url(url):\n",
    "    r = get(url)\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "    tables = soup.find_all('table')\n",
    "    df = pd.read_html(str(tables))[0]\n",
    "    new_header = df.iloc[0] #grab the first row for the header\n",
    "    df = df[1:] #take the data unless the header row\n",
    "    df.columns = new_header\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "872efe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uses requests to get html and searches for all tables, returning them as a df with no header\n",
    "def get_df_from_url_noheader(url):\n",
    "    r = get(url)\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "    tables = soup.find_all('table')\n",
    "    df = pd.read_html(str(tables))[0]\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a31b5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opens html file and extracts table to df from table id\n",
    "def html_to_df_from_tableid(file_name, table_id):\n",
    "    with open('NCAA/{}.html'.format(file_name), encoding=\"utf-8\") as f:\n",
    "        page = f.read()\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "        soup.find('tr', class_= 'over_header').decompose() #Removes 'over_header' element of table\n",
    "        table = soup.find(id=table_id)\n",
    "        df = pd.read_html(str(table))[0]\n",
    "    return df\n",
    "\n",
    "#Uses URL to get df based on table id\n",
    "def html_to_df_from_tableid_no_download(url, table_id):\n",
    "    r = get(url)\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "    table = soup.find(id=table_id)\n",
    "    df = pd.read_html(str(table))[0]\n",
    "    return df\n",
    "\n",
    "#Uses URL to get df based on table class\n",
    "def html_to_df_from_table_class_no_download(url):\n",
    "    r = get(url)\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "    table = soup.find('table', {'class' : [\"tablesaw\", \"tablesaw-swipe\", \"tablesaw-sortable\"]})\n",
    "    df = pd.read_html(str(table))[0]\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e52bfb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removes Positons from player names as to only include player name in 'Player' Collumn\n",
    "def clean_player_name(series):\n",
    "    new_series = []\n",
    "    for item in series:\n",
    "        index = 0\n",
    "        for char in reversed(item):\n",
    "            if char.capitalize() == char or char == '-':\n",
    "                index = index - 1\n",
    "            else:\n",
    "                break\n",
    "        new_series.append(item[:index].strip())\n",
    "    return(new_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b100d2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts from inches to cm so model can interpret data\n",
    "def inch_to_cm(series):\n",
    "    new_series = []\n",
    "    for measurement in series:\n",
    "        measurement = measurement.strip()\n",
    "        feet = float(measurement[0])\n",
    "        inches = float(measurement[2:-1])\n",
    "        height_inches = feet*12 + inches\n",
    "        height_cm = round(height_inches * 2.54, 2)\n",
    "        new_series.append(height_cm)\n",
    "    return(new_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fac48f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removes instances in dataset where last character is a non-int value \n",
    "def remove_last_char(series):\n",
    "    new_series = []\n",
    "    for item in series:\n",
    "        if isinstance(item, str):\n",
    "            if item[-1] == '‘' or item[-1] == '′' or item[-1] == '%':\n",
    "                new_series.append(item[:-1])\n",
    "            else:\n",
    "                new_series.append(item)\n",
    "        else:\n",
    "            new_series.append(item)\n",
    "    return(new_series)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db216038",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a list of dfs of combine measurements for each year in years_custom\n",
    "dfs = []\n",
    "for year in years_custom: \n",
    "    temp_df = get_df_from_url(url_draft.format(year))\n",
    "    temp_df['Year'] = year\n",
    "    dfs.append(temp_df)\n",
    "\n",
    "#Renames columns in each dataframe to be the same across all dataframes \n",
    "#Specifically 2013 has different formatting for columns and player names which is fixed through this code\n",
    "column_names = {'Name' : 'Player',\n",
    "                'Ht w/o Shoes': 'Height w/o Shoes',\n",
    "                'Ht w/ Shoes': 'Height w/ Shoes',\n",
    "                'Reach': 'Standing Reach',\n",
    "                'Body Fat': 'Body Fat %'\n",
    "               }\n",
    "\n",
    "for df in dfs:\n",
    "    if 'Name' in df:\n",
    "        df.rename(columns = column_names, inplace = True)\n",
    "\n",
    "#Removes positions from player names where applicable\n",
    "for df in dfs[1:]:\n",
    "    df['Player'] = clean_player_name(df['Player'])\n",
    "\n",
    "#Merges dataframes\n",
    "draft_main_df = pd.concat(dfs)\n",
    "\n",
    "#Removes rows with null values as well as various typos from source data\n",
    "for column in draft_main_df:\n",
    "    draft_main_df = draft_main_df[draft_main_df[column] != '–']\n",
    "    draft_main_df = draft_main_df[draft_main_df[column] != '0']\n",
    "    draft_main_df[column] = remove_last_char(draft_main_df[column])\n",
    "    draft_main_df[column] = remove_last_char(draft_main_df[column])\n",
    "    draft_main_df = draft_main_df.dropna()\n",
    "\n",
    "#Removes data points containing typos to annoying to fix \n",
    "bad_data = ['Cody Martin',\n",
    "    'Trae Young',\n",
    "    'Darius Bazley',\n",
    "    'Carsen Edwards',\n",
    "    'Tacko Fall',\n",
    "    'Jaylen Hands',\n",
    "    'Mitch McGary',\n",
    "    'Rakeem Christmas',\n",
    "    'Brian Bowen',\n",
    "    'Semaj Christon',\n",
    "    'Stephen Zimmerman']\n",
    "\n",
    "for name in bad_data:\n",
    "    \n",
    "    draft_main_df = draft_main_df[draft_main_df['Player'] != name]\n",
    "\n",
    "#Converts inches to cm in columns where unit formatting is unreadable\n",
    "columns_for_conversion = ['Height w/o Shoes', \n",
    "                          'Height w/ Shoes', \n",
    "                          'Wingspan', \n",
    "                          'Standing Reach']\n",
    "\n",
    "for column in columns_for_conversion:\n",
    "    draft_main_df[column] = inch_to_cm(draft_main_df[column])\n",
    "\n",
    "\n",
    "#Write dataframe to a csv\n",
    "draft_main_df.to_csv('Combine_Measurements.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9de75615",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a list of dfs of draft data for each year in years_custom\n",
    "dfs_draft = []\n",
    "for year in years_custom:\n",
    "    url = url_draft_summary.format(year)\n",
    "    dfs = pd.read_html(url)\n",
    "    temp_list = [dfs[0], dfs[1]]  #Note that https://basketball.realgm.com/ will occasionally change webpage layout and therefore indexing may need to be adjusted\n",
    "    temp_df = pd.concat(temp_list)\n",
    "    temp_df['Year'] = year\n",
    "    dfs_draft.append(temp_df)\n",
    "\n",
    "\n",
    "\n",
    "#Gets relevant columns from the draft data dataframes\n",
    "dfs_clean = []\n",
    "relevant_columns = ['Player', 'Pick', 'Age', 'Year']\n",
    "for df in dfs_draft:\n",
    "    df = df[relevant_columns]\n",
    "    dfs_clean.append(df)\n",
    "\n",
    "draft_data_df = pd.concat(dfs_clean)\n",
    "final = pd.merge(draft_main_df, draft_data_df, on=['Player', 'Year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81be247a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor url in list_of_urls:\\n    get_html_api(url)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scrapes around 200 webpages from basketball reference, Saves files in /NCAA\n",
    "#Can run it if you would like but Selenium may crash every once in a while\n",
    "#If this occurs I would suggest indexing list_of_urls to avoid scraping pages already scraped before the crash and re-running\n",
    "'''\n",
    "for url in list_of_urls:\n",
    "    get_html_api(url)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "406a57bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Goes through each HTML file in NCAA and extracts table as a df and adds it to the dfs_college list\n",
    "#Cell will take around 90s to run\n",
    "dfs_college = []\n",
    "for url in list_of_urls:\n",
    "    for year in years_custom:\n",
    "        fname = Path('./NCAA/{}.html'.format((generate_name(url, year))))\n",
    "        if os.path.isfile(fname):\n",
    "            temp_df = html_to_df_from_tableid(generate_name(url, year), NCAA_id)\n",
    "            temp_df['Year'] = year\n",
    "            dfs_college.append(temp_df)\n",
    "\n",
    "#Adds all college players to a single df and gets relevant collumns\n",
    "college_total = pd.concat(dfs_college)\n",
    "relevant_columns = ['Player', 'Pos', 'G','MP','FG%','2P%','3P%','FT%','PTS.1','TRB.1','AST.1','STL.1','BLK.1','PER','WS','BPM', 'Year']\n",
    "columns_to_rename = {'PTS.1': 'PTS',\n",
    "                    'AST.1': 'AST',\n",
    "                    'STL.1': 'STL',\n",
    "                    'BLK.1': 'BLK',\n",
    "                    'TRB.1': 'TRB'}\n",
    "\n",
    "college_total_clean = college_total.loc[:,relevant_columns]\n",
    "college_total_clean.rename(columns = columns_to_rename , inplace = True)\n",
    "\n",
    "#Merges df containing college player stats and NBA draft info to a single df which is written to total.csv for analysis\n",
    "total = pd.merge(college_total_clean, final, on=['Player', 'Year'])\n",
    "total['3P%'] = total['3P%'].fillna(0)\n",
    "total.dropna()\n",
    "\n",
    "total.to_csv('total.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
